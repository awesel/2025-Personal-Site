<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Coffee and Machine Learning</title>
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400&display=swap"
      rel="stylesheet"
    />
    <style>
      body {
        margin: 0;
        padding: 0;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
          "Helvetica Neue", Arial, sans-serif;
        background-color: #ffffff;
        color: #000000;
        display: flex;
        justify-content: center;
        align-items: flex-start;
        min-height: 100vh;
        padding: 20px 0;
      }

      .container {
        max-width: 800px;
        padding: 20px;
        margin-left: 20px;
        margin-right: 20px;
      }

      h1 {
        font-size: 2em;
        margin-bottom: 0.3em;
      }

      h2 {
        font-size: 1.5em;
        margin-top: 1.5em;
        margin-bottom: 0.5em;
      }

      p {
        font-size: 1em;
        line-height: 1.6;
        margin-bottom: 1em;
      }

      .back-link {
        margin-top: 2em;
      }

      .back-link a {
        text-decoration: none;
        color: #0000ee;
      }

      .back-link a:hover {
        text-decoration: underline;
      }

      .author {
        font-size: 1em;
        font-style: italic;
        color: #444;
        margin-bottom: 0.1em;
      }

      ol {
        padding-left: 1.5em;
      }

      li {
        margin-bottom: 1.5em;
      }

      li strong {
        display: block;
        margin-bottom: 0.3em;
      }

      li p {
        margin-top: 0.5em;
      }

      a {
        color: #0000ee;
        text-decoration: none;
      }

      a:hover {
        text-decoration: underline;
      }

      ul.day-meta {
        list-style-type: none;
        padding-left: 1.2em;
        margin-top: 0.3em;
        margin-bottom: 0.7em;
      }
      ul.day-meta li {
        position: relative;
        margin-bottom: 0.1em;
        padding-left: 1em;
        font-size: 1em;
        line-height: 1.4;
      }
      ul.day-meta li::before {
        content: "\2013"; /* en dash */
        position: absolute;
        left: 0;
        color: #444;
        font-size: 1.1em;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>Coffee and Machine Learning</h1>
      <div class="author">Andrew Wesel</div>

      <p>
        I have a month that I'm spending in Los Angeles before going back to the
        Bay to work for the summer. Over the school year, I've developed a
        pretty long reading list of machine learning papers that I've wanted to
        read but haven't had the time. This month, I'm going to try to catch up!
      </p>
      <p>
        This writing will contain my own notes and is not intended to be super
        interesting to read. I'm only putting it online because part of this
        project is to talk with my friends about what I'm reading, which I think
        having these notes online will help me do.
      </p>
      <p>
        Last note: I have a lot of free time this month. This readings seem to
        be taking me 1-2 hours, so I will do them at coffee shops. In addition
        to tracking what papers I read, I will also track what coffees I drink,
        and how many people I run into serendipitously.
      </p>
      <h2>Calendar</h2>
      <ol>
        <li>
          <strong
            >June 9: Neural networks and the multivariable Chain Rule</strong
          >
          <ul class="day-meta">
            <li>From: Math 51 Textbook Appendix</li>
            <li>Vanilla Latte from Alfred Coffee in Brentwood</li>
            <li>Number of friends seen: 6</li>
          </ul>
          <p>
            This is my third time reading this appendix. Every time I read it, I
            learn something new, which I think either demonstrates the passage's
            high quality of writing, or my poor reading comprehension. Thinking
            of neural networks as a composition of functions really makes it
            clear that machine learning is really just an optimization problem
            in multiple variables. Reading this passage also prompted me to
            learn more about different optimizers- after implementing logistic
            regression for CS109 last week, I feel much more comfortable with
            the differences between gradient descent, SGD, mini-batch SGD, and
            Adam. Last thought: I cannot believe this paper was written in 1950.
            Claude Shannon was DIFFERENT.
            <a
              href="https://www.paradise.caltech.edu/ist4/lectures/shannonchess1950.pdf"
              target="_blank"
              >A Chess-Playing Machine</a
            >
          </p>
        </li>
        <li>
          <strong
            >June 10: Language Model Fine-Tuning on Scaled Survey Data for
            Predicting Distributions of Public Opinions</strong
          >
          <ul class="day-meta">
            <li>
              From:
              <a href="https://arxiv.org/pdf/2502.16761" target="_blank"
                >arXiv:2502.16761</a
              >
            </li>
            <li>Black Drip Coffee from La La Land in Santa Monica</li>
            <li>Number of friends seen: 1</li>
          </ul>
          <p>
            Last month, I fine-tuned Qwen to improve accuracy at estimating
            cross-national survey distributions. I showed that project to one of
            my professors, who recommended I read this paper as they do a
            similar experiment. It seems like their prompt-completion pairs
            looked something like:
            <br />
            &nbsp;&nbsp;&nbsp;&nbsp;- Prompt: You're going to fill in a
            distribution of survey responses. Answer in brackets, where each
            value corresponds to a percentage of respondents who chose that
            option. Subpopulation: AGE 65+. Survey: How important, if at all, is
            being a gun owner to your overall identity? Options: ['Very
            important', 'Somewhat important', 'Not too important', 'Not at all
            important', 'Refused']
            <br />
            &nbsp;&nbsp;&nbsp;&nbsp;- Completion: [0.28108718, 0.235006,
            0.27790644, 0.20600038]
            <br />
            and they fine-tuned the LLM to just output the distribution of
            answers. In my experiment, we trained on reasoning traces and the
            distribution of answers, not just the distribution of answers. I am
            somewhat surprised that their method worked because, during
            fine-tuning on a dataset of this structure, the model can only
            memorize answers, not learn problem-solving strategies for this type
            of question. Clearly, there's some sauce here though that I don't
            fully understand.
          </p>
          <p>
            They use Wasserstein (Earth Mover's) distance and KL divergence as
            their distance metrics to evaluate how similar two distributions
            are. My experiment used Jensen-Shannon distance, and I only did that
            because
            <a href="https://arxiv.org/abs/2306.16388" target="_blank">
              this Anthropic paper</a
            >
            did it. I think it's bad form that I blindly adopted the same
            strategy without thinking about it, and I'd love to investigate
            further whether one of these distance metrics is better than the
            other. We had some issues with JS distance being impacted by the
            number of options in the question, even when we filled in the number
            of options in the "log" field in Python.
          </p>
          <p>
            Also, I found it interesting that they experience diminishing
            marginal returns when fine-tuning on larger datasets. Specifically
            they got 75% of best results when fine-tuning on only 25% of the
            training dataset. This might validate my project's use of only 1300
            train examples. I was quite worried about having a small training
            dataset, but we ended up with quite good results. This validates our
            observation.
          </p>
          <p>
            Last thing: I think the use case they identify for this technology
            is intriguing. They talk about social scientists using this
            technology to probe different survey questions and populations when
            designing experiments, and identifying which subpopulations might
            need to be oversampled. I like that they emphasized that they aren't
            intending to replace human data.
          </p>
        </li>
        <li>
          <strong>June 11: Attention Is All You Need</strong>
          <ul class="day-meta">
            <li>
              From:
              <a href="https://arxiv.org/abs/1706.03762" target="_blank"
                >arXiv:1706.03762</a
              >
            </li>
            <li>Coffee: Nola from Blue Bottle in Brentwood</li>
            <li>Number of friends seen: Not specified</li>
          </ul>
          <p>
            I've learned about attention in class, but I've never actually read
            the original paper. I assume that, in my life, I may read this paper
            multiple times. This is just a first pass.
          </p>
          <p>
            The first thing I investigated (just upon reading the abstract) was
            the difference between encoder-decoder and decoder-only models. I
            have heard people say that GPT and Llama are "decoder-only
            transformers" but didn't really know what that meant until I saw
            this paper talk about encoder-decoder transformers. Encoder-decoder
            models rely on two neural networks (one each for encoding and
            decoding). The encoder network creates a set of vectors representing
            the input sequence. The decoder then generates tokens by looking at
            the encoder's representation, and the tokens before it in the output
            sequence. Decoder-only models don't distinguish between an input and
            output sequence, they consider the input to be the beginning of the
            output sequence, and try to complete it. This means that the
            input/output distinction is implicit and learned by the model, not a
            "real" attribute of a token.
          </p>
          <p>
            Next is their discussion of parallizability and why transformer
            models parallelize better than RNNs. RNNs have a hidden state that
            is updated based on each token, requiring every prior token to be
            done processing before processing the next. While inference in
            (non-diffusion) transformers is autoregressive/sequential, training
            is easily parallelizable. Transformers embed all tokens in a
            sequence, compute key, query, and value matrices (three huge
            matmuls), then compute the attention scores with softmax(query *
            keyT / sqrt(d)) and the ultimate "attention-refined representation"
            of each token by multiplying by the value matrix. Because these
            matricies depend only on the original embedings of the input and
            target sequences, not on intermediate outputs, they can be
            parallelized easily, unlike with RNNs. They also give a similar
            argument for why transformers are more efficient thant CNNs, which
            require stacking multiple convolutional layers to gain full context
            for a sequence.
          </p>
          <p>
            The paper also explains the 'multi-head attention' mechanism. A
            single set of key, query, and value matricies can theoretically
            encode a lot of information due to high dimensionality, but there
            must be some limit. Because learned parameters often have some
            stochasticity, one can't be certain that a given instance of
            attention values is necessarily 'correct' or ideal. Multi-head
            attention resolves this issue by computing multiple 'heads' of
            attention and concateinating the results. In technical language,
            "Multi-head attention allows the model to jointly attend to
            information from different representation subspaces at different
            positions," which in plain english means different heads can learn
            different things and form a rich representation when combined. Each
            head is of a smaller dimension than the default QKV matricies, so
            the total computation ends up being similar.
          </p>
          <p>
            Since the attention mechanism only looks at how much each token
            matters to another, not their relative position, one needs to
            deliberately add positional information. So then FinalEmbed(token) =
            DefaultEmbed(token) + Position(token). The vector Position(token)
            has each value i computed with either a sin or cos depending on
            whether token is at an even or odd position through the function
            sin(token_position/10000^(2i/d)). I don't quite understand why they
            alternate between sin and cos, or why they don't just use integer
            relationships (i.e. -3 position embed means one token is 3 behind
            another). Someday I will figure this out. I assume it has to do with
            numbers getting too large, or because of the way dot
            products/multiplication processes these values.
          </p>
          <p>
            The rest of the paper details their experiment, and how successful
            it was. I'll note just a few of their observations: more attention
            heads is not always better, too-small attention size hurts quality,
            and bigger models are better. This paper required a lot intuition
            about linear algebra. While the actual math might have been less
            intense than the linear algebra textbook passage, it's a lot less
            explicit, They expect readers to be clear about matrix operations
            and don't hand-hold through any operations, which is to be expected
            of higher-level research. But of course, this made reading this
            passage quite challenging. I'll come back to this paper in a few
            weeks and see whether I can get anything more out of it.
          </p>
        </li>
        <li>
          <strong>June 12: Backpropogation </strong>
          <ul class="day-meta">
            <li>
              From:
              <a
                href="https://www.youtube.com/watch?v=VkHfRKewkWw"
                target="_blank"
                >Welch Labs on YouTube</a
              >
            </li>
            <li>Coffee: Cappuccino from Joe and the Juice in Brentwood</li>
            <li>Number of friends seen: None. Sad!</li>
          </ul>
          <p>
            I've found the ML videos from this channel to be quite easy to
            understand. This dovetails nicely with the earlier reading from the
            Math 51 textbook. It reiterates that backpropogation is just an
            application of the chain rule.
          </p>
          <p>
            I knew that, in language modeling, the gradient is found by setting
            the "correct" word from a piece of training text's probability to 1
            and everything else to zero (one-hot encoding), and subtracting the
            predicted values from that. This video goes over all of the calculus
            required to derive that simple expression. I was surprised how well
            it simplifies!
          </p>
        </li>
        <li>
          <strong
            >June 13: Adversarial Examples Are Not Bugs, They Are
            Features</strong
          >
          <ul class="day-meta">
            <li>
              From:
              <a href="https://arxiv.org/abs/1905.02175" target="_blank"
                >arXiv:1905.02175</a
              >
            </li>
            <li>Coffee: Lavazza K-Cup from my Keurig</li>
            <li>Number of friends seen: None :(</li>
          </ul>
          <p>
            This paper was written by my data science professor! I saw it being
            cited in a Google red-teaming report, so I thought it must be pretty
            relevant. The main argument is that, sometimes, ML models learn
            "non-robust features" that are somehow highly predictive but
            not truly representative of the label they are classifying.
            The researchers look at image classification, which is an
            illustrative example of this phenomenon. Viewing any natural photo,
            the model will perform fantastically. But if one looks into the
            features themselves, there are ways to edit photos to make the model
            misclassify them, even when a human would not be able to tell the
            difference.
          </p>
          <p>
            One of the main takeaways is that there sometimes exists a tradeoff
            between a model being accurate and it being comprehensible to
            humans. If we want a Dog Classifier to accurately classify dogs, the
            easiest way to do it might be to find some abstract pattern in the
            noise of an image, not to learn what a dog actually is (ears, legs,
            etc). This is a problem because, if the model's "definition" of a
            dog is not the same as a human's, humans can't easily understand
            what the model is doing.
          </p>
          <p>
            I saw this a similar discussion in the DeepSeek-R1 paper.
            DeepSeek-R1-Zero found, during reinforcement learning, that the best
            way to solve math problems is to write a solution that alternates
            between different languages, presumably English and Chinese. Rather
            than let the model achieve maximum accuracy, the researchers added a
            reward for the model to write solutions in a single language. If I
            was given this choice, I don't know what I would do. To achieve
            superintelligence, I feel that we have to, almost by definition, let
            AI do some stuff that we don't fully understand. This is one
            interpretation of The Bitter Lesson- we shouldn't put human
            knowledge on a pedestal, we should just run huge searches and see
            what comes up.
          </p>
          <p>
            On the other hand, I totally understand why, for safety reasons, we
            might want to be able to easily observe the manner by which a
            language model reaches its conclusions. Obviously, I don't really
            have a solution to this trade-off, I think it's important to
            recognize that it exists, though.
          </p>
        </li>
        <li>
          <strong
            >June 14: Godel Escher Bach Preface</strong>
        </li>
          <p>
            I started this book today. Since I'm a nerdy computer guy interested in consciousness, I feel like I am supposed to read this sometime soon. My discrete math professor recommended it to the class (some of our problems were based on the book) and I really enjoyed that class.
          </p>
          <p> The preface clarifies some common misconceptions about the book, it's true meaning, and its structure. I think it's promising that the book focuses so much on metaphor (from what I understand, the dialogues are all metaphors for themes). I know that I learn well from metaphor, so this book may be very good for me.</p>
        </li>
        <li>
            <strong
              >June 15: Introduction</strong>
          </li>
            <p> This chapter concerns paradoxes in self-referential statements. There's a strong connection being drawn between mathematical statements that reference themselves paradoxically, Escher drawings that illustrate paradoxes, musical self reference (theme and variation), and the way that conscious beings reference themselves. This observation is already quite profound to me.
              </p>
          </li>
          <li>
            <strong
              >June 16: Chapter 1</strong>
          </li>
            <p> Here, we go over the MU system, which was part of a CS 103 lecture. We learn about infinities and Zeno's paradoxes. We also learn about reasoning "about" versus "within" a system, with some surprisingly relevant concepts for thinking about the economy, consciousness, etc. In the following dialog, we follow characters humorously realizing that, when writing proofs, they shouldn't write "D. If clause A and clause B, then clause C," because that would require them to write a similar clause E, ad infinitum. I like this book.
              </p>
          </li>
        
      </ol>
      </ol>

      <div class="back-link">
        <a href="index.html">← Back to Home</a>
      </div>
    </div>
  </body>
</html>
